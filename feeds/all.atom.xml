<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>/home/dprince</title><link href="https://dprince.github.io/" rel="alternate"></link><link href="https://dprince.github.io/feeds/all.atom.xml" rel="self"></link><id>https://dprince.github.io/</id><updated>2025-01-26T09:00:00-05:00</updated><entry><title>OpenStack Operator Initialization Resource</title><link href="https://dprince.github.io/openstack-operator-initialization-resource.html" rel="alternate"></link><published>2025-01-26T09:00:00-05:00</published><updated>2025-01-26T09:00:00-05:00</updated><author><name>Dan Prince</name></author><id>tag:dprince.github.io,2025-01-26:/openstack-operator-initialization-resource.html</id><summary type="html">&lt;p&gt;OpenStack Operator Initialization Resource&lt;/p&gt;</summary><content type="html">&lt;h2&gt;RHOSO a set of Kubernetes operators to deploy OpenStack&lt;/h2&gt;
&lt;p&gt;Over the past few years, I’ve had the privilege of working on a team at Red Hat, where we’ve developed a comprehensive set of Kubernetes
operators specifically designed to deploy OpenStack. At this point we have a total of over 20 operators. While these operators are each
focussed on deploying their respective openstack services our desire is to deploy them as a single product called RHOSO (Red Hat OpenStack on OpenShift).&lt;/p&gt;
&lt;h2&gt;Deploying with OLM, our journey so far&lt;/h2&gt;
&lt;p&gt;To deploy our operators, we utilize OLM (Operator Lifecycle Manager), a package manager that simplifies the process of deploying operators on your Kubernetes cluster. OLM is included by default with OpenShift and offers several features that enable controlled installation and upgrade of operators over time. Initially, we deployed all our operators using a single OLM bundle, which managed synchronization of all operators and presented them as a single product with multiple components. However, we soon encountered an issue with the OLM bundle size limit.&lt;/p&gt;
&lt;p&gt;Our subsequent iteration, which marked the product’s general availability (GA), involved deploying the product using OLM with over 22 bundles and implementing a simple dependency mechanism to manage their deployment. This approach has proven to be effective, but we still face a few challenges:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We desire only one operator product to be visible in the OpenShift Console (UI), while still allowing all operator package manifests to be displayed on the command line.&lt;/li&gt;
&lt;li&gt;If an older version of the openstack-operator is installed, it may still install the latest version of service operators, resulting in a mixed set of operators. While explicit pinning mechanisms exist, they require manual intervention and could be tedious.&lt;/li&gt;
&lt;li&gt;There is still a concern regarding the bundle size for the openstack-operator. We need sufficient space in the bundle to accommodate multiple versions of our Custom Resource Definitions (CRDs) in the future.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There’s hope on the horizon with the upcoming release of OLM v1, which promises to address the bundle size issue. However, we can’t afford to wait as we urgently need a solution for OCP 4.16.&lt;/p&gt;
&lt;h2&gt;A new initialization resource&lt;/h2&gt;
&lt;p&gt;The upcoming FR2 release of the RHOSO (Red Hat OpenStack on OpenShift) product will deploy the service operators via a new initialization resource.&lt;/p&gt;
&lt;p&gt;An initialization resource is a custom resource is a k8s custom resource that is used to "initialize" something in the k8s cluster early
so that the operators themselves can do their work. Many k8s operators and products use a similar pattern. OLM you can indicate that you have
an initialization-resource by adding an annotation to your CSV (Cluster Service Version). The new annotation for our OpenStack operator looks like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;operatorframework.io/initialization-resource: &amp;#39;{&amp;quot;apiVersion&amp;quot;:&amp;quot;operator.openstack.org/v1beta1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;OpenStack&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;name&amp;quot;:&amp;quot;openstack&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;openstack-operators&amp;quot;},&amp;quot;spec&amp;quot;:{}}&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;After adding this section to the CSV the OpenShift Console shows the new initialization resource as being required and looks something like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="OpenStack initialization resource UI" src="/images/openstack_operator_initialization.png"&gt;&lt;/p&gt;
&lt;h2&gt;What the OpenStack initialization resource currently does&lt;/h2&gt;
&lt;p&gt;Once created the new OpenStack initialization works by bootstrapping/installing the following resources for all of the OpenStack operators:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CRDs (custom resource definitions)&lt;/li&gt;
&lt;li&gt;RBAC permissions&lt;/li&gt;
&lt;li&gt;Creates and manages all the OpenStack operator deployments&lt;/li&gt;
&lt;li&gt;Webhooks. Webhook certificates are now installed via cert-manager directly instead of via OLM&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All the these resources get installed directly from the openstack-operator container itself which means we no longer have any size limits. Additionally
we can now maintain all our resources as a single product entry in the OLM catalog.&lt;/p&gt;
&lt;h2&gt;For developers, scaling service operator deployments to 0&lt;/h2&gt;
&lt;p&gt;In the past our project had individual OLM CSVs for each OpenStack service operator. But now there is only a single CSV for the entire
set of operators. The OpenStack service operators now run as k8s Deployments which are owned by the OpenStack initialization resource.&lt;/p&gt;
&lt;p&gt;One commmon use case for developers is to install all the OpenStack operators and scale down a single operator's replicas to 0 and then
run a modified version locally with code changes to functionally test things. The new way to do that is:&lt;/p&gt;
&lt;p&gt;1) Edit the CSV for the OpenStack Operator and set the replicas for the controller-operator to 0. This step prevents the initialization resource from overwriting any changes we make to owned resources.
2) Edit the Deployment for the OpenStack service operator you wish to disable and set its replicas to 0.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;The initialization resource really helps streamline the deployment of our operators. It also puts us in control of how operators get updated and should 
consolidate the management of things in the future. I could see us adding a few features to the initialization resource to help prevent operator updates
until a maintenance window and perform other setup type tasks.&lt;/p&gt;</content><category term="OpenStack"></category><category term="openstack"></category><category term="openshift"></category></entry><entry><title>K8s CRD size limits</title><link href="https://dprince.github.io/k8s-crd-size-limits.html" rel="alternate"></link><published>2025-01-22T09:00:00-05:00</published><updated>2025-01-22T09:00:00-05:00</updated><author><name>Dan Prince</name></author><id>tag:dprince.github.io,2025-01-22:/k8s-crd-size-limits.html</id><summary type="html">&lt;p&gt;K8s CRD size limits&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Size concerns when deploying a large software project on k8s&lt;/h2&gt;
&lt;p&gt;Working on a large k8s deployment project for OpenStack we've hit a few limits with our
k8s operators. I recently decided to take a closer look at CRD size limits,
how to properly calculate them, and what to watch out for in both the short and long term.&lt;/p&gt;
&lt;h2&gt;How we use Custom Resource Definitions&lt;/h2&gt;
&lt;p&gt;A Custom Resource Definition (CRD) is a way you can extend the k8s API and we use it heavily to deploy OpenStack.
One of our deployment goals has been to streamline deployment around Controlplane and Dataplane concepts.
This split works well since the Controlplane's OpenStack services get deployed natively on k8s via operators and the Dataplane services get deployed on RHEL via Ansible. The Dataplane has a few different CRDs to help drive Ansible via an Operator native workflow. The Controlplane CRD encapsulates the configuration, scaling, orchestration, and updating of over 20 "service operators". Our Controlplane CRD has ended up growing quite large so it is the one this article will focus on.&lt;/p&gt;
&lt;h2&gt;The metadata.annotations limit&lt;/h2&gt;
&lt;p&gt;The first limit we hit with the ControlPlane was:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;metadata.annotations: Too long: must have at most 262144 bytes
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This had to do with the size limit on annotations in k8s (262144 bytes) and the fact that our k8s operators all default
to using the kubectl default for client side apply. When using client side apply to create or update a CRD it stores
a full version of that CRD in an annotation. This is so it can compare it for future updates. This isn't strictly
a CRD size limit, though, and can be avoided by simply using --server-side with 'kubectl apply' commands. It also
doesn't get triggered if you deploy your CRDs via OLM (Operator Lifecycle Manager) which is our primary deployment use case.
Our CRD size continued to grow...&lt;/p&gt;
&lt;h2&gt;The Etcd limit&lt;/h2&gt;
&lt;p&gt;Search online 'what is the CRD size limit' and you'll likely get an answer around 1MB. Keep digging
and you'll find that the actual limit depends on how your Etcd cluster gets configured when deployed.
The upstream default request size limit for etcd is currently 1.5MB https://etcd.io/docs/v3.5/dev-guide/limit/. The Red Hat OpenShift k8s distribution happens to use this same limit. OpenShift also does not support changing this limit. This is the hard limit on CRD size. Also, It appears the k8s API also checks for this same limit but it is the underlying etcd storage limit which is the driver.&lt;/p&gt;
&lt;p&gt;So this is the limit, but how close are we to actually hitting it?&lt;/p&gt;
&lt;h2&gt;Checking CRD size: Yaml vs Json&lt;/h2&gt;
&lt;p&gt;Our stock Yaml version of a Controlplane is around 900k (with descriptions disabled). But that isn't how etcd stores it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$&lt;span class="w"&gt; &lt;/span&gt;etcdctl&lt;span class="w"&gt; &lt;/span&gt;get&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/kubernetes.io/apiextensions.k8s.io/customresourcedefinitions/openstackcontrolplanes.core.openstack.org&amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;--print-value-only&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;true&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;wc&lt;span class="w"&gt; &lt;/span&gt;-c
&lt;span class="m"&gt;293507&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This is because Etcd stores it in Json format without any whitespace. So a better way to check our CRD size quickly on the command line appears to be something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$&lt;span class="w"&gt; &lt;/span&gt;yq&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;eval&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;-o&lt;span class="o"&gt;=&lt;/span&gt;json&lt;span class="w"&gt; &lt;/span&gt;core.openstack.org_openstackcontrolplanes.yaml&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;jq&lt;span class="w"&gt; &lt;/span&gt;-c&lt;span class="w"&gt; &lt;/span&gt;.&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;wc&lt;span class="w"&gt; &lt;/span&gt;-c
&lt;span class="m"&gt;291405&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It does appear the serialization through the API slightly modifies the Json ordering and size vs what we get on the command line, but the difference isn't significant. Probably good to keep a large buffer anyway when avoiding size limits.&lt;/p&gt;
&lt;p&gt;This is good news in that we should have plenty of space now before we hit the 1.5MB limit. Roughly enough space to store 5 versions of our current CRD size.
This last part is important because CRD definitions contain multiple versions so we'll need to have enough space to upgrade to a new version at some point.&lt;/p&gt;
&lt;p&gt;Our plan is to implement a check to make sure we don't go above a certain threshold. This should ensure we always have enough space for at least 2 versions and can therefore always upgrade from 1 version to the next.&lt;/p&gt;
&lt;h2&gt;Where does all the space go&lt;/h2&gt;
&lt;p&gt;The graph below gives a rough idea which CRDs are consuming the most space. As you can clearly see the RabbitMQ struct makes up the biggest chunk in our ControlPlane CRD. This is because the rabbitmq-operator includes a nested podTemplateSpec definition which ends up being quite large. We unfortunately didn't catch how large this struct was until after GA so there are plans to streamline, or possibly get rid of the struct in our ControlPlane CRD when we create a new version. This alone would save a good chunk of space in our CRD.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Controlplane CRD Sizes" src="/images/controlplane_crd_size.png"&gt;&lt;/p&gt;
&lt;p&gt;Note: When generating this graph I rolled entries for some of the smallest services like OpenStackclient, Memcached, TLS configuration, DNS, and Redis into the 'other' category. I also added in the top level extraMounts as the size there is still significant and fits into the analysis below.&lt;/p&gt;
&lt;h2&gt;Things you can do to minimize CRD size&lt;/h2&gt;
&lt;p&gt;If you also have gotten into large CRD territory here are some ideas that might help:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;disabling descriptions when generating larger CRDs. Our docs and reference architecture cover the descriptions anyway, so the savings here is significant.&lt;/li&gt;
&lt;li&gt;avoiding the use of large nested structs like podTemplateSpec where possible&lt;/li&gt;
&lt;li&gt;where large nested structs are needed use streamlined versions of them. Our project doesn't need all the fields for extraMounts, for example. In places where podTemplateSpec is used we might also benefit from a more streamlined version of this nested struct.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;The vision for our initial Controlplane was a composition of OpenStack resources. This was largely driven by a desire to simplify the deployment workflow and centrally control orchestration for OpenStack services around minor and major updates.&lt;/p&gt;
&lt;p&gt;The good news is for the time being we have space left to grow, add new features, and perform upgrades. And we have some options to shrink things further. But the Controlplane is one of the largest CRDs I've seen and going forward we need to keep a close eye on the size. It comes with the territory though as OpenStack is after all a large project.&lt;/p&gt;</content><category term="OpenShift"></category><category term="openstack"></category><category term="openshift"></category><category term="k8s"></category></entry><entry><title>Keystone Operator Deploy/Upgrade on OpenShift</title><link href="https://dprince.github.io/keystone-operator-deployupgrade-on-openshift.html" rel="alternate"></link><published>2020-01-10T17:00:00-05:00</published><updated>2020-01-10T17:00:00-05:00</updated><author><name>Dan Prince</name></author><id>tag:dprince.github.io,2020-01-10:/keystone-operator-deployupgrade-on-openshift.html</id><summary type="html">&lt;p&gt;Keystone deploy and upgrade with an OpenShift/Kubernetes Operator&lt;/p&gt;</summary><content type="html">&lt;p&gt;My &lt;a href="https://dprince.github.io/keystone-kubernetes-operator-installation.html"&gt;last post&lt;/a&gt; was about installing an Operator for Keystone. In this post I go over how to use the same &lt;a href="https://github.com/openstack-k8s-operators/keystone-operator"&gt;Keystone Operator&lt;/a&gt; to
deploy and then upgrade a working Keystone API on OpenShift using containers from the &lt;a href="https://www.rdoproject.org/"&gt;RDO project&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Creating a route to access Keystone API&lt;/h1&gt;
&lt;p&gt;The first thing is to create a route that will be used to access the Keystone API outside (external to) the OpenShift/Kubernetes cluster. Create yaml file called route.yaml that looks something like this (swapping in values for your local project and domain):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;apiVersion&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;route.openshift.io/v1&lt;/span&gt;
&lt;span class="nt"&gt;kind&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Route&lt;/span&gt;
&lt;span class="nt"&gt;metadata&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;keystone&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;namespace&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;test&lt;/span&gt;
&lt;span class="nt"&gt;spec&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;host&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;keystone-test.apps.test.dprince&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;path&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;/&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;to&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;kind&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Service&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;keystone&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;port&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;targetPort&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;api&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Once you have created the file create the route with the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;oc&lt;span class="w"&gt; &lt;/span&gt;create&lt;span class="w"&gt; &lt;/span&gt;-f&lt;span class="w"&gt; &lt;/span&gt;route.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;NOTE: We create the route first because so that we can pass the dns hostname being used as a parameter when creating the Keystone API. This is used to "bootstrap" the Keystone service endpoints. Also note that OpenShift routes support a variety of TLS options (including edge and end-to-end configurations). For simplicity we are keeping with standard http for this demo.&lt;/p&gt;
&lt;p&gt;Once you have completed these steps you should have an OpenShift route created that points to http://keystone-test.apps.test.dprince. This will route traffic through the OpenShift load balancer to the internal keystone service running on OpenShift.&lt;/p&gt;
&lt;h1&gt;Deploying Keystone API&lt;/h1&gt;
&lt;p&gt;The Keystone Operator gave us a custom CRD which can be used to create Keystone objects within the cluster. Create a YAML file representing the Keystone API we want to create like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;apiVersion&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;keystone.openstack.org/v1&lt;/span&gt;
&lt;span class="nt"&gt;kind&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;KeystoneApi&lt;/span&gt;
&lt;span class="nt"&gt;metadata&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;keystone&lt;/span&gt;
&lt;span class="nt"&gt;spec&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;adminPassword&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;foobar123&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;containerImage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;docker.io/tripleostein/centos-binary-keystone:current-tripleo&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;replicas&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;1&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;databasePassword&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;foobar123&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;databaseHostname&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;openstack-db-mariadb&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;# used for keystone-manage bootstrap endpoints&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;apiEndpoint&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;http://keystone-test.apps.test.dprince/&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;# used to create the DB schema&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;databaseAdminUsername&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;root&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;databaseAdminPassword&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;foobar123&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;mysqlContainerImage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;docker.io/tripleomaster/centos-binary-mariadb:current-tripleo&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Save this file as a YAML file called keystone.yaml and then create the resource with the following command.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;oc&lt;span class="w"&gt; &lt;/span&gt;create&lt;span class="w"&gt; &lt;/span&gt;-f&lt;span class="w"&gt; &lt;/span&gt;keystone.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;NOTE: The assumption here is that you are already running a MariaDB instances in your cluster. Eventually maybe we'll have an Operator for that too, and we could even abstract away some of the DB parameters above once that happens.&lt;/p&gt;
&lt;p&gt;Once the command completes the Keystone Operator will start the deployment. It goes through several phases including: creating the Keystone database, creating a &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"&gt;Kubernetes Deployment&lt;/a&gt; resource within the cluster, and then bootstrapping the Keystone installation. You can watch the stdout of your Keystone Operator pod if you want to see it happen live. All of the commands should take about a minute or so to complete and once it is finished you should have a live working Keystone installation.&lt;/p&gt;
&lt;h1&gt;Test it&lt;/h1&gt;
&lt;p&gt;Now its time to actually use and test the installation. Create a file called stackrc that looks like this (again swap in values for your own environment if you are trying this):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nb"&gt;export&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;OS_AUTH_URL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;http://keystone-test.apps.test.dprince/
&lt;span class="nb"&gt;export&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;OS_PASSWORD&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;foobar123
&lt;span class="nb"&gt;export&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;OS_USERNAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;admin
&lt;span class="nb"&gt;export&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;OS_TENANT_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;admin
&lt;span class="nb"&gt;export&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;COMPUTE_API_VERSION&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;.1
&lt;span class="nb"&gt;export&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;OS_NO_CACHE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;True
&lt;span class="nb"&gt;export&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;OS_IDENTITY_API_VERSION&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;OS_PROJECT_DOMAIN_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Default
&lt;span class="nb"&gt;export&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;OS_USER_DOMAIN_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Default
&lt;span class="nb"&gt;export&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;OS_AUTH_VERSION&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now lets run a few sample commands to test the installation and show the version of the service:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;openstack&lt;span class="w"&gt; &lt;/span&gt;token&lt;span class="w"&gt; &lt;/span&gt;issue
openstack&lt;span class="w"&gt; &lt;/span&gt;versions&lt;span class="w"&gt; &lt;/span&gt;show
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you used the same containerImage from the OpenStack Train release the 'versions show' command above should display the identity service at version 3.12.&lt;/p&gt;
&lt;h1&gt;Upgrade it&lt;/h1&gt;
&lt;p&gt;Next we'll upgrade the Keystone deployment to the OpenStack Train release. This will be a live rolling upgrade, although I'm not sure Keystone officially supports this it demonstrates the capability and seems to work fine.&lt;/p&gt;
&lt;p&gt;To fire off the upgrade run the following OpenShift command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;oc&lt;span class="w"&gt; &lt;/span&gt;patch&lt;span class="w"&gt; &lt;/span&gt;keystoneapi&lt;span class="w"&gt; &lt;/span&gt;keystone&lt;span class="w"&gt; &lt;/span&gt;--type&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;json&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;-p&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;[{&amp;quot;op&amp;quot;: &amp;quot;replace&amp;quot;, &amp;quot;path&amp;quot;: &amp;quot;/spec/containerImage&amp;quot;, &amp;quot;value&amp;quot;:&amp;quot;docker.io/tripleotrain/centos-binary-keystone:current-tripleo&amp;quot;}]&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This upgdates the keystoneapi resource we initially deployed to use a new containerImage for
the Train release. The Operator watches for changes to keystoneapi resources and reacts to
them immediately and takes the appropriate actions to run the DB sync for the upgraded
and do a rolling update to a newer API container. Again you can watch the stdout of the
Keystone Operator container if you want to see it happen live. It should take less than a minute
to finish.&lt;/p&gt;
&lt;h1&gt;Prove that it works&lt;/h1&gt;
&lt;p&gt;Once the upgrade finishes we'll run another versions command to see what is returned.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;openstack&lt;span class="w"&gt; &lt;/span&gt;versions&lt;span class="w"&gt; &lt;/span&gt;show
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If everything ran correctly you should see 3.13 running the latest OpenStack Train release.&lt;/p&gt;
&lt;h1&gt;Some final thoughts&lt;/h1&gt;
&lt;p&gt;Hopefully this gives an idea of what it is like to deploy and manage an application with a Kubernetes/OpenShift Operator. There is still a lot to be implemented to make the Keystone Operator feature complete but it already demos quite nicely I think.&lt;/p&gt;</content><category term="OpenStack"></category><category term="openstack"></category><category term="kubernetes"></category><category term="openshift"></category></entry><entry><title>Keystone Kubernetes Operator Installation</title><link href="https://dprince.github.io/keystone-kubernetes-operator-installation.html" rel="alternate"></link><published>2019-07-19T17:00:00-04:00</published><updated>2019-07-19T17:00:00-04:00</updated><author><name>Dan Prince</name></author><id>tag:dprince.github.io,2019-07-19:/keystone-kubernetes-operator-installation.html</id><summary type="html">&lt;p&gt;OpenStack Keystone Kubernetes Operator Installation&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="Installing a Keystone k8s Operator" src="https://dprince.github.io/images/openshift_keystone_operator_installed.png"&gt;&lt;/p&gt;
&lt;p&gt;Since Open Infrastructure Summit in Denver earlier this year I've spent some time ramping up on &lt;a href="https://coreos.com/operators/"&gt;Kubernetes Operators&lt;/a&gt; and Golang. Perhaps “some time” is an understatement, this is more of a coaxed deep dive into some technology which I knew existed but hadn’t paid proper attention too yet. (hint: I was asked to look into this). In the past few weeks I've created a working version of a &lt;a href="https://github.com/openstack-k8s-operators/keystone-operator"&gt;Keystone Kubernetes Operator&lt;/a&gt; in Go.&lt;/p&gt;
&lt;p&gt;The Keystone Operator is based on the &lt;a href="https://github.com/operator-framework/operator-sdk"&gt;operator-sdk&lt;/a&gt;  which provides a nice way to cookie cut the Operator application structure. It also provides some nice benefits for releasing and installing the Operator in your Kubernetes cluster. There are several ways to install an Operator which make them really flexible.&lt;/p&gt;
&lt;h1&gt;For local development&lt;/h1&gt;
&lt;p&gt;If you've built your k8s Operator with the operator-sdk you've got access to some niceties and one is the 'up local' command. Below is an example of how to use this with the current &lt;a href="https://github.com/openstack-k8s-operators/keystone-operator/blob/master/deploy/crds/keystone_v1_keystoneapi_crd.yaml"&gt;Keystone CRD&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;kubectl&lt;span class="w"&gt; &lt;/span&gt;create&lt;span class="w"&gt; &lt;/span&gt;-f&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;deploy/crds/keystone_v1_keystoneapi_crd.yaml&lt;span class="o"&gt;]&lt;/span&gt;
operator-sdk&lt;span class="w"&gt; &lt;/span&gt;up&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;local&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;These commands create the required CRD and then launch a working version of the k8s Operator. The last command there stays running in your local terminal and logs any output from your k8s Operator. As long as you leave it running you can run API commands to create
k8s custom resources provided by your Operator. It’s important to note that by default this command runs with your local credentials and the Operator has access to any permissions that you have as well. There are some options to fine tune 'up local' though so it runs with different creds (--kubeconfig) or in an alternate namespace (--namespace) if you need them too. See (operator-sdk --help for more)&lt;/p&gt;
&lt;h1&gt;Deploying an Operator on the CLI&lt;/h1&gt;
&lt;p&gt;Because Operators are containers that are deployed in your Kubernetes cluster, deploying them is building and publishing a container. The operator-sdk has a few commands that make this streamlined:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;operator-sdk&lt;span class="w"&gt; &lt;/span&gt;build&lt;span class="w"&gt; &lt;/span&gt;openstack-keystone&lt;span class="w"&gt; &lt;/span&gt;--image-builder&lt;span class="w"&gt; &lt;/span&gt;buildah
podman&lt;span class="w"&gt; &lt;/span&gt;tag&lt;span class="w"&gt; &lt;/span&gt;&amp;lt;image&lt;span class="w"&gt; &lt;/span&gt;id&amp;gt;&lt;span class="w"&gt; &lt;/span&gt;quay.io/openstack-k8s-operators/keystone-operator:devel
podman&lt;span class="w"&gt; &lt;/span&gt;push&lt;span class="w"&gt; &lt;/span&gt;quay.io/openstack-k8s-operators/keystone-operator:devel
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Once you've uploaded your container to a registry you can deploy it into your cluster on the CLI using files in the 'deploy' directory. Using either kubectl or oc you can deploy them like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;kubectl&lt;span class="w"&gt; &lt;/span&gt;create&lt;span class="w"&gt; &lt;/span&gt;-f&lt;span class="w"&gt; &lt;/span&gt;deploy/service_account.yaml
kubectl&lt;span class="w"&gt; &lt;/span&gt;create&lt;span class="w"&gt; &lt;/span&gt;-f&lt;span class="w"&gt; &lt;/span&gt;deploy/role.yaml
kubectl&lt;span class="w"&gt; &lt;/span&gt;create&lt;span class="w"&gt; &lt;/span&gt;-f&lt;span class="w"&gt; &lt;/span&gt;deploy/role_binding.yaml
kubectl&lt;span class="w"&gt; &lt;/span&gt;create&lt;span class="w"&gt; &lt;/span&gt;-f&lt;span class="w"&gt; &lt;/span&gt;deploy/operator.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h1&gt;Deploying an Operator via OLM&lt;/h1&gt;
&lt;p&gt;Lastly there is a cool new &lt;a href="https://github.com/operator-framework/operator-lifecycle-manager"&gt;Operator Lifecycle Manager&lt;/a&gt; that takes care of some of the k8s Operator
management for you. I won't get into all the details here but the benefits of using the OLM
are automatic k8s Operator deployment from channels, dependency management between operators (a bit crude at the moment but promising), and integration with OperatorHub.&lt;/p&gt;
&lt;p&gt;In order to integrate with OLM you need a CSV (clusterserviceversion) and the operator-sdk makes this fairly easy too via the 'operator-sdk olm-catalog gen-csv' command. When you initially create a CSV some things are automatically generated but a few others need to be manually specified. Say you've already crafted a 0.0.1 CSV and you want to release 0.0.2 you could do it like this via the operator-sdk.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;operator-sdk&lt;span class="w"&gt; &lt;/span&gt;olm-catalog&lt;span class="w"&gt; &lt;/span&gt;gen-csv&lt;span class="w"&gt; &lt;/span&gt;--csv-channel&lt;span class="w"&gt; &lt;/span&gt;default&lt;span class="w"&gt; &lt;/span&gt;--operator-name&lt;span class="w"&gt; &lt;/span&gt;keystone-operator&lt;span class="w"&gt; &lt;/span&gt;--csv-version&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.0.2&lt;span class="w"&gt; &lt;/span&gt;--verbose&lt;span class="w"&gt; &lt;/span&gt;--from-version&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.0.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Generated CSV's can be deployed on the CLI just like any other custom resource.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;kubectl&lt;span class="w"&gt; &lt;/span&gt;create&lt;span class="w"&gt; &lt;/span&gt;-f&lt;span class="w"&gt; &lt;/span&gt;deploy/olm-catalog/keystone-operator/0.0.1/keystone-operator.v0.0.1.clusterserviceversion.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This will take care of checking (and possibly installing) dependencies for roles, service accounts,
and ultimately if everything checks out it will fire off a Deployment to install the k8s Operator in the cluster. It even has a spot for a nice icon there which will show up in your k8s console if you've got the OLM components installed there.&lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;As someone who has worked on OpenStack deployment for the last few years, k8s Operators check a lot of the boxes that I'm passionate about. Assuming your goal is to deploy on k8s, when using k8s Operators your deployment tools are built using the same framework as your production applications (i.e k8s API's). The tools have an API that is provided as a CRD just like any other k8s resources you are using and can be managed as such. Cutting a new release of the deployment tools is as simple as building and publishing a container. Lastly, given the tools above the integration with the dev/test process is very streamlined. I think I could get used to this.&lt;/p&gt;</content><category term="OpenStack"></category><category term="openstack"></category><category term="kubernetes"></category><category term="openshift"></category></entry><entry><title>Docker Puppet</title><link href="https://dprince.github.io/docker-puppet.html" rel="alternate"></link><published>2017-01-25T09:00:00-05:00</published><updated>2017-01-25T09:00:00-05:00</updated><author><name>Dan Prince</name></author><id>tag:dprince.github.io,2017-01-25:/docker-puppet.html</id><summary type="html">&lt;p&gt;Docker puppet: how we generate config files for containers&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today TripleO leverages Puppet to help configure and manage the
deployment of OpenStack services. As we move towards using Docker
one of the big questions people have is how will we generate config files
for those containers. We'd like to continue to make use of our mature
configuration interfaces (Heat parameters, Hieradata overrides, Puppet modules)
to allow our operators to seamlessly take the step towards a fully
containerized deployment. &lt;/p&gt;
&lt;p&gt;With the recently added composable service we've got
everything we need. This is how we do it...&lt;/p&gt;
&lt;h1&gt;Install puppet into our base container image&lt;/h1&gt;
&lt;p&gt;Turns out the first thing you need of you want to generate config files
with Puppet is well... puppet. TripleO uses containers from the Kolla
project and by default they do not install Puppet. In the past TripleO
uses an 'agent container' to manage the puppet installation requirements.
This worked okay for the compute role (a very minimal set of services)
but doesn't work as nicely for the broader set of OpenStack services because
packages need to be pre-installed into the 'agent' container in order for
config file generation to work correctly (puppet overlays the default config
files in many cases). Installing packages for all of OpenStack and its
requirements into the agent container isn't ideal.&lt;/p&gt;
&lt;p&gt;Enter TripleO composable services (thanks Newton!). TripleO now supports
composability and Kolla typically has individual containers
for each service so it turns out the best way to generate config files
for a specific service is to use the container for the service itself. We
do this in two separate runs of a container: one to create config files, and
the second one to launch the service (bind mounting/copying in the configs).
It works really well.&lt;/p&gt;
&lt;p&gt;But we still have the issue of how do we get puppet into all of our Kolla
containers. We were happy to discover that Kolla supports a template-overrides
mechanism (A jinja template) that allows you to customize how containers
are built. This is how you can use that mechanism to add puppet into
the Centos base image used for all the OpenStack docker containers generated
by Kolla build scripts.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$&lt;span class="w"&gt; &lt;/span&gt;cat&lt;span class="w"&gt; &lt;/span&gt;template-overrides.j2
&lt;span class="o"&gt;{&lt;/span&gt;%&lt;span class="w"&gt; &lt;/span&gt;extends&lt;span class="w"&gt; &lt;/span&gt;parent_template&lt;span class="w"&gt; &lt;/span&gt;%&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt;%&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;base_centos_binary_packages_append&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;puppet&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;%&lt;span class="o"&gt;}&lt;/span&gt;

kolla-build&lt;span class="w"&gt; &lt;/span&gt;--base&lt;span class="w"&gt; &lt;/span&gt;centos&lt;span class="w"&gt; &lt;/span&gt;--template-override&lt;span class="w"&gt; &lt;/span&gt;template-overrides.j2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h1&gt;Control the Puppet catalog&lt;/h1&gt;
&lt;p&gt;A puppet manifest in TripleO can do a lot of things like installing packages,
configuring files, starting a service, etc. For containers we only want to
generate the config files. Furthermore we'd like to do this without having to
change our puppet modules.&lt;/p&gt;
&lt;p&gt;One mechanism we use is the --tags option for 'puppet apply'. This option
allows you to specify which resources within a given puppet manifest (or catalog) should be executed. It works really nicely to allow you to select what
you want out of a puppet catalog.&lt;/p&gt;
&lt;p&gt;An example of this is listed below where we have a manifest to create
a '/tmp/foo' file. When we run the manifest with the 'package' tag
(telling it to only install packages) it does nothing at all.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$&lt;span class="w"&gt; &lt;/span&gt;cat&lt;span class="w"&gt; &lt;/span&gt;test.pp&lt;span class="w"&gt; &lt;/span&gt;
file&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/tmp/foo&amp;#39;&lt;/span&gt;:
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nv"&gt;content&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&amp;gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;bar&amp;#39;&lt;/span&gt;,
&lt;span class="o"&gt;}&lt;/span&gt;
$&lt;span class="w"&gt; &lt;/span&gt;puppet&lt;span class="w"&gt; &lt;/span&gt;apply&lt;span class="w"&gt; &lt;/span&gt;--tags&lt;span class="w"&gt; &lt;/span&gt;package&lt;span class="w"&gt; &lt;/span&gt;test.pp
Notice:&lt;span class="w"&gt; &lt;/span&gt;Compiled&lt;span class="w"&gt; &lt;/span&gt;catalog&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;undercloud.localhost&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;environment&lt;span class="w"&gt; &lt;/span&gt;production&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.10&lt;span class="w"&gt; &lt;/span&gt;seconds
Notice:&lt;span class="w"&gt; &lt;/span&gt;Applied&lt;span class="w"&gt; &lt;/span&gt;catalog&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.02&lt;span class="w"&gt; &lt;/span&gt;seconds
$&lt;span class="w"&gt; &lt;/span&gt;cat&lt;span class="w"&gt; &lt;/span&gt;/tmp/foo
cat:&lt;span class="w"&gt; &lt;/span&gt;/tmp/foo:&lt;span class="w"&gt; &lt;/span&gt;No&lt;span class="w"&gt; &lt;/span&gt;such&lt;span class="w"&gt; &lt;/span&gt;file&lt;span class="w"&gt; &lt;/span&gt;or&lt;span class="w"&gt; &lt;/span&gt;directory
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h1&gt;When --tags doesn't work&lt;/h1&gt;
&lt;p&gt;The --tags option of 'puppet apply' doesn't always give us the
behavior we are after which is to generate only config files. Some puppet
modules have custom resources with providers that can execute commands anyway.
This might be a mysql query or an openstackclient command to create a keystone
endpoint. Remember here that we are trying to re-use puppet modules from
our baremetal configuration and these resources are expected to be in
our manifests... we just don't want them to run at the time we are generating
config files. So we need an alternative mechanism to suppress (noop out)
these offending resources.&lt;/p&gt;
&lt;p&gt;To do this we've started using a custom built noop_resource function that
exists in puppet-tripleo. This function dynamically configures a default
provider for the named resource. For mysql this ends up looking like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[&amp;#39;Mysql_datadir&amp;#39;, &amp;#39;Mysql_user&amp;#39;, &amp;#39;Mysql_database&amp;#39;, &amp;#39;Mysql_grant&amp;#39;, &amp;#39;Mysql_plugin&amp;#39;].each |String $val| { noop_resource($val) }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Running a puppet manifest with this at the top will noop out any of the named
resource types and they won't execute. Thus allowing puppet apply to complete
and finish generating the config files within the specified manifest.&lt;/p&gt;
&lt;p&gt;The good news is most of our services don't require the noop_resource in
order to generate config files cleanly. But for those that do the interface
allows us to effectively disable the resources we don't want to execute.&lt;/p&gt;
&lt;h1&gt;Putting it all together: docker-puppet.py&lt;/h1&gt;
&lt;p&gt;Bringing everything together in tripleo-heat-templates to create
one container configuration interface that will allow us to
configurably generate per-service config files. It looks like this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;manifest: the puppet manifest to use to generate config files (Thanks to composable services this is now per service!)&lt;/li&gt;
&lt;li&gt;puppet_tags: the puppet tags to execute within this manifest&lt;/li&gt;
&lt;li&gt;config_image: the docker image to use to generate config files. Generally we use the same image as the service itself.&lt;/li&gt;
&lt;li&gt;config_volume: where to output the resulting config tree (includes /etc/ and some other directories).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And then we've created a custom tool to drive this per-service configuration
called docker-puppet.py. The tool supports using the information above in a
Json file format drive generation of the config files in a single action.&lt;/p&gt;
&lt;p&gt;It ends up working like this:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://youtu.be/lqDWkBzetFk"&gt;Video demo: Docker Puppet&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And thats it. Our config interfaces are intact. We generate the config files
we need. And we get to carry on with our efforts to deploy with containers.&lt;/p&gt;
&lt;p&gt;Links:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://git.openstack.org/cgit/openstack/puppet-tripleo/tree/lib/puppet/parser/functions/noop_resource.rb"&gt;puppet noop_resource function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://review.openstack.org/#/c/416421/29/docker/docker-puppet.py"&gt;docker_puppet.py source code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="TripleO"></category><category term="tripleo"></category><category term="openstack"></category><category term="puppet"></category></entry></feed>